description: >
  BiLSTM with assertions.

parent_config: './conf/qa/abstract_qa.yaml'

name: 'bilstm_assertion_reader'
reader: 'modular_assertion_qa_reader'

assertion_dir: '/data/assertion_store'
assertion_limit: 50
assertion_sources: ['conceptnet']
no_reading: False

reading_module:
- input: 'text'
  module: 'lstm'
  name: 'reading'
  with_projection: True
  activation: 'relu'

dropout: &dropout 0.2
repr_dim: 150
max_span_size: 16

model:
  encoder_layer:

  - input: 'support'
    module: 'dense'
    activation: 'tanh'
    name: 'embedding_projection'

  - input: 'support'
    output: 'support_conv'
    module: 'conv_glu'
    name: 'context_encoder'
    conv_width: 5
    num_layers: 1

  - input: ['support', 'support_conv']
    output: 'support'
    module: 'concat'

  - input: 'question'
    module: 'dense'
    activation: 'tanh'
    name: 'embedding_projection'

  - input: 'question'
    output: 'question_conv'
    module: 'conv_glu'
    name: 'context_encoder'
    conv_width: 5
    num_layers: 1

  - input: ['question', 'question_conv']
    output: 'question'
    module: 'concat'

  - input: 'support'
    dependent: 'question'
    module: 'attention_matching'
    attn_type: 'diagonal_bilinear'
    with_sentinel: True  # we gate the attention with an additional scalar sentinel because what we retrieve might actually not be what we were looking for because (softmax) attn always retrieves something
    scaled: True

  - input: 'support'
    module: 'lstm'
    activation: 'tanh'
    with_projection: True
    dropout: *dropout

  - input: 'question'
    module: 'lstm'
    with_projection: True
    activation: 'tanh'
    dropout: *dropout

  answer_layer:
    module: 'bilinear'
