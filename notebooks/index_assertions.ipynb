{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shelve\n",
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "index_fn = \"/data/assertion_shelve\"\n",
    "max_len = 30\n",
    "\n",
    "#nlp = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
    "nlp = spacy.load('en', parser=False, entity=False, matcher=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "object2assertions = dict()\n",
    "subject2assertions = dict()\n",
    "assertions = dict()\n",
    "\n",
    "def lemmatized(tokens, start, end):\n",
    "    return \" \".join(t.lemma_ for t in tokens if t.idx < end and t.idx >= start)\n",
    "\n",
    "\n",
    "ctr = 0\n",
    "\n",
    "def process_line(l, source):\n",
    "    global ctr\n",
    "    #e.g.:  'pair of compasses synonym compass\\t0:17\\t26:33'\n",
    "    try:\n",
    "        l = l.strip()\n",
    "        if \"\\t\" not in l:\n",
    "            return\n",
    "\n",
    "        [text, subject_spans, object_spans] = l.split(\"\\t\")\n",
    "        \n",
    "        if text.count(\" \") < max_len:\n",
    "            tokens = nlp(text)\n",
    "            subjects = [lemmatized(tokens, int(s[:s.index(\":\")]), int(s[s.index(\":\") + 1:]))\n",
    "                        for s in subject_spans.split(\",\")]\n",
    "            objects = [lemmatized(tokens, int(s[:s.index(\":\")]), int(s[s.index(\":\") + 1:]))\n",
    "                       for s in object_spans.split(\",\")]\n",
    "\n",
    "            if len(subjects) == 1 and len(objects) == 1 and subjects[0] == objects[0]:\n",
    "                return\n",
    "\n",
    "            ctr += 1\n",
    "\n",
    "            for subject in subjects:\n",
    "                if subject not in subject2assertions[source]:\n",
    "                    subject2assertions[source][subject] = set()\n",
    "                subject2assertions[source][subject].add(str(ctr))\n",
    "\n",
    "            for obj in objects:\n",
    "                if obj not in object2assertions[source]:\n",
    "                    object2assertions[source][obj] = set()\n",
    "                object2assertions[source][obj].add(str(ctr))\n",
    "            assertions[str(ctr)] = text\n",
    "        #else:\n",
    "            #print(\"skipping assertion bigger than %d...\" % max_len)\n",
    "    except ValueError:\n",
    "        print(\"Could not process line: \" + l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ppdb_L assertions...\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "Processing conceptnet assertions...\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(index_fn):\n",
    "    import shutil\n",
    "    shutil.rmtree(index_fn)\n",
    "    \n",
    "os.mkdir(index_fn)  \n",
    "\n",
    "\n",
    "def process_source(name, path):\n",
    "    global assertions, ctr\n",
    "    print(\"Processing %s assertions...\" % name)\n",
    "    object2assertions[name] = dict()\n",
    "    subject2assertions[name] = dict()\n",
    "    with open(path) as f:\n",
    "        for l in f:\n",
    "            process_line(l, name)\n",
    "            if ctr % 100000 == 0:\n",
    "                print(\"%d\" % ctr)\n",
    "                db.update(assertions)\n",
    "                del assertions\n",
    "                assertions = dict()\n",
    "\n",
    "with shelve.open(os.path.join(index_fn, 'assertions.shelve')) as db:\n",
    "    #process_source(\"dbpedia_type\", \"/run/media/diwe01/Data3/wiki/dbpedia/type_assertions.txt\") \n",
    "    #process_source(\"microsoft_type\", \"/run/media/diwe01/Data2/corpora/concepts/concepts_microsoft/data-concept/type_assertions.txt\") \n",
    "    #process_source(\"simple_wikipedia_firstsent\", \"/run/media/diwe01/Data3/wiki/simple_abstracts/wiki_assertions.txt\") \n",
    "    #process_source(\"en_wikt\", \"/run/media/diwe01/Data3/wiki/en_wikt/meaning_assertions.txt\") \n",
    "    #process_source(\"wikipedia_firstsent\", \"/run/media/diwe01/Data3/wiki/en_abstracts/wiki_assertions.txt\") \n",
    "    process_source(\"ppdb_L\", \"/run/media/diwe01/Data2/corpora/concepts/ppdb/assertions_pos.txt\") \n",
    "    process_source(\"conceptnet\", \"/run/media/diwe01/Data2/corpora/concepts/conceptnet/assertions.txt\")\n",
    "\n",
    "    db.update(assertions)\n",
    "    \n",
    "\n",
    "with open(os.path.join(index_fn, 'subject2assertions.pkl'), \"wb\") as f:\n",
    "    pickle.dump(subject2assertions, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(os.path.join(index_fn, 'object2assertions.pkl'), \"wb\") as f:\n",
    "    pickle.dump(object2assertions, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
