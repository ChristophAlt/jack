{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a new model with Jack "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we focus on the minimal steps required to implement a new model from scratch using Jack.\n",
    "\n",
    "We will implement a simple Bi-LSTM baseline for extractive question answering.\n",
    "The architecture is as follows:\n",
    "- Words of question and support are embedded using random embeddings (not trained)\n",
    "- Both word and question are encoded using a bi-directional LSTM\n",
    "- The question is summarized by averaging its token representations\n",
    "- A feedforward NN scores each of the support tokens to be the start of the answer\n",
    "- A feedforward NN scores each of the support tokens to be the end of the answer\n",
    "\n",
    "In order to implement a Jack reader, we define three modules:\n",
    "- **Input Module**: Responsible for mapping `QASetting`s to numpy array assoicated with `TensorPort`s\n",
    "- **Model Module**: Defines the TensorFlow graph\n",
    "- **Output Module**: Converting the network output to the output of the system. In our case, this involves extracting the answer string from the context. We will use the existing `XQAOutputModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First change dir to jack parent\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import re\n",
    "\n",
    "from jack.core import *\n",
    "from jack.io.embeddings import Embeddings\n",
    "from jack.util.vocab import *\n",
    "from jack.readers.extractive_qa.shared import XQAPorts, XQAOutputModule\n",
    "from jack.readers.extractive_qa.util import prepare_data, stack_and_pad\n",
    "from jack.readers.extractive_qa.util import tokenize\n",
    "from jack.tf_fun.rnn import birnn_with_projection\n",
    "from jack.util import tfutil\n",
    "from jack.util.map import numpify\n",
    "\n",
    "_tokenize_pattern = re.compile('\\w+|[^\\w\\s]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ports\n",
    "\n",
    "All communication between input, model and output modules happens via `TensorPort`s (see `jack/core/tensorport.py`).\n",
    "Normally, you should try to reuse ports wherever possible to be able to reuse modules as well.\n",
    "If you need a new port, however, it is also straight-forward to define one.\n",
    "For this tutorial, we will define most ports here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedded_question = TensorPort(tf.float32, [None, None, None], \"embedded_question_flat\",\n",
    "                               \"Represents the embedded question\",\n",
    "                               \"[Q, max_num_question_tokens, N]\")\n",
    "# or reuse FlatPorts.Misc.embedded_question\n",
    "\n",
    "question_length = TensorPort(tf.int32, [None], \"question_length_flat\",\n",
    "                             \"Represents length of questions in batch\",\n",
    "                             \"[Q]\")\n",
    "# or reuse FlatPorts.Input.question_length\n",
    "\n",
    "embedded_support = TensorPort(tf.float32, [None, None, None], \"embedded_support_flat\",\n",
    "                              \"Represents the embedded support\",\n",
    "                              \"[S, max_num_tokens, N]\")\n",
    "# or reuse FlatPorts.Misc.embedded_support\n",
    "\n",
    "support_length = TensorPort(tf.int32, [None], \"support_length_flat\",\n",
    "                            \"Represents length of support in batch\",\n",
    "                            \"[S]\")\n",
    "# or reuse FlatPorts.Input.support_length\n",
    "\n",
    "answer_span = TensorPort(tf.int32, [None, 2], \"answer_span_target_flat\",\n",
    "                         \"Represents answer as a (start, end) span\", \"[A, 2]\")\n",
    "# or reuse FlatPorts.Prediction.answer_span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reuse the `XQAOutputModule`, we'll use existing ports defined in `XQAPorts` for the `char_token_offset` and the predictions.\n",
    "We'll also use the `Ports.loss` port, because the the JTR training code expects this port as output of the model module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorport 'token_char_offsets'\n",
      "  dtype: <dtype: 'int32'>\n",
      "  shape: [None, None]\n",
      "  doc_string: Character offsets of tokens in support.\n",
      "  shape_string: [S, support_length]\n",
      "Tensorport 'start_scores_flat'\n",
      "  dtype: <dtype: 'float32'>\n",
      "  shape: [None, None]\n",
      "  doc_string: Represents start scores for each support sequence\n",
      "  shape_string: [S, max_num_tokens]\n",
      "Tensorport 'end_scores_flat'\n",
      "  dtype: <dtype: 'float32'>\n",
      "  shape: [None, None]\n",
      "  doc_string: Represents end scores for each support sequence\n",
      "  shape_string: [S, max_num_tokens]\n",
      "Tensorport 'answer_span_prediction_flat'\n",
      "  dtype: <dtype: 'int32'>\n",
      "  shape: [None, 2]\n",
      "  doc_string: Represents answer as a (start, end) span\n",
      "  shape_string: [A, 2]\n",
      "Tensorport 'loss'\n",
      "  dtype: <dtype: 'float32'>\n",
      "  shape: [None]\n",
      "  doc_string: Represents loss on each instance in the batch\n",
      "  shape_string: [batch_size]\n"
     ]
    }
   ],
   "source": [
    "print(XQAPorts.token_char_offsets.get_description())\n",
    "print(XQAPorts.start_scores.get_description())\n",
    "print(XQAPorts.end_scores.get_description())\n",
    "print(XQAPorts.span_prediction.get_description())\n",
    "print(Ports.loss.get_description())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Module\n",
    "\n",
    "The input module is responsible for converting `QASetting` instances to numpy\n",
    "arrays, which are mapped to `TensorPort`s. Essentially, we are building a\n",
    "feed dict used for training and inference. Note, there are input modules for\n",
    "several readers that can easily be reused when your model requires the same\n",
    "pre-processing and input as another model. Similarly, this is true for the\n",
    "OutputModule. In case you can reuse those modules it is enough to simply\n",
    "implement your ModelModule (see below). See `jack/readers/implementations.py`\n",
    "how different readers re-use the same modules.\n",
    "\n",
    "You could implement the `InputModule` interface, but in many cases it'll be\n",
    "easier to inherit from `OnlineInputModule`. Doing this, we need to:\n",
    "- Define the output `TensorPort`s of our input module\n",
    "- Implement the preprocessing (e.g. tokenization, mapping to embedding vectors, ...). The result of this step is one *annotation* per instance, e.g. a `dict`.\n",
    "- Implement batching. Given a list of annotations, you need to define how to build the feed dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyInputModule(OnlineInputModule):\n",
    "\n",
    "    def __init__(self, shared_resources):\n",
    "        \"\"\"The module is initialized with a `shared_resources`.\n",
    "\n",
    "        For the purpose of this tutorial, we will only use the `vocab` property\n",
    "        which provides the embeddings. You could also pass arbitrary\n",
    "        configuration parameters in the `shared_resources.config` dict.\n",
    "        \"\"\"\n",
    "        self.vocab = shared_resources.vocab\n",
    "        self.emb_matrix = self.vocab.emb.lookup\n",
    "\n",
    "    # We will now define the input and output TensorPorts of our model.\n",
    "\n",
    "    @property\n",
    "    def output_ports(self):\n",
    "        return [embedded_question,           # Question embeddings\n",
    "                question_length,             # Lengths of the questions\n",
    "                embedded_support,            # Support embeddings\n",
    "                support_length,              # Lengths of the supports\n",
    "                XQAPorts.token_char_offsets  # Character offsets of tokens in support, used for in ouput module\n",
    "               ]\n",
    "\n",
    "    @property\n",
    "    def training_ports(self):\n",
    "        return [answer_span]                 # Answer span, one for each question\n",
    "\n",
    "    # Now, we implement our preprocessing. This involves tokenization,\n",
    "    # mapping to token IDs, mapping to to token embeddings,\n",
    "    # and computing the answer spans.\n",
    "\n",
    "    def _get_emb(self, idx):\n",
    "        \"\"\"Maps a token ID to it's respective embedding vector\"\"\"\n",
    "        if idx < self.emb_matrix.shape[0]:\n",
    "            return self.vocab.emb.lookup[idx]\n",
    "        else:\n",
    "            # <OOV>\n",
    "            return np.zeros([self.vocab.emb_length])\n",
    "\n",
    "    def preprocess(self, questions, answers=None, is_eval=False):\n",
    "        \"\"\"Maps a list of instances to a list of annotations.\n",
    "\n",
    "        Since in our case, all instances can be preprocessed independently, we'll\n",
    "        delegate the preprocessing to a `_preprocess_instance()` method.\n",
    "        \"\"\"\n",
    "\n",
    "        if answers is None:\n",
    "            answers = [None] * len(questions)\n",
    "\n",
    "        return [self._preprocess_instance(q, a)\n",
    "                for q, a in zip(questions, answers)]\n",
    "\n",
    "    def _preprocess_instance(self, question, answers=None):\n",
    "        \"\"\"Maps an instance to an annotation.\n",
    "\n",
    "        An annotation contains the embeddings and length of question and support,\n",
    "        token offsets, and optionally answer spans.\n",
    "        \"\"\"\n",
    "\n",
    "        has_answers = answers is not None\n",
    "\n",
    "        # `prepare_data()` handles most of the computation in our case, but\n",
    "        # you could implement your own preprocessing here.\n",
    "        q_tokenized, q_ids, q_length, s_tokenized, s_ids, s_length, \\\n",
    "        word_in_question, token_offsets, answer_spans = \\\n",
    "            prepare_data(question, answers, self.vocab,\n",
    "                         with_answers=has_answers,\n",
    "                         max_support_length=100)\n",
    "\n",
    "        # For both question and support, we'll fill an embedding tensor\n",
    "        emb_support = np.zeros([s_length, self.emb_matrix.shape[1]])\n",
    "        emb_question = np.zeros([q_length, self.emb_matrix.shape[1]])\n",
    "        for k in range(len(s_ids)):\n",
    "            emb_support[k] = self._get_emb(s_ids[k])\n",
    "        for k in range(len(q_ids)):\n",
    "            emb_question[k] = self._get_emb(q_ids[k])\n",
    "\n",
    "        # Now, we build the annotation for the question instance. We'll use a\n",
    "        # dict that maps from `TensorPort` to numpy array, but this could be\n",
    "        # any data type, like a custom class, or a named tuple.\n",
    "\n",
    "        annotation = {\n",
    "            question_length: q_length,\n",
    "            embedded_question: emb_question,\n",
    "            support_length: s_length,\n",
    "            embedded_support: emb_support,\n",
    "            XQAPorts.token_char_offsets: token_offsets\n",
    "        }\n",
    "\n",
    "        if has_answers:\n",
    "            # For the purpose of this tutorial, we'll only use the first answer, such\n",
    "            # that we will have exactly as many answers as questions.\n",
    "            annotation[answer_span] = list(answer_spans[0])\n",
    "\n",
    "        return numpify(annotation, keys=[support_length, question_length,\n",
    "                                         XQAPorts.token_char_offsets, answer_span])\n",
    "\n",
    "    def create_batch(self, annotations, is_eval, with_answers):\n",
    "        \"\"\"Now, we need to implement the mapping of a list of annotations to a feed dict.\n",
    "        \n",
    "        Because our annotations already are dicts mapping TensorPorts to numpy\n",
    "        arrays, we only need to do padding here.\n",
    "        \"\"\"\n",
    "\n",
    "        return {key: stack_and_pad([a[key] for a in annotations])\n",
    "                for key in annotations[0].keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Module.\n",
    "\n",
    "The model module defines the TensorFlow computation graph.\n",
    "It takes input module outputs as inputs and produces outputs such as the loss\n",
    "and outputs required by hte output module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyModelModule(TFModelModule):\n",
    "\n",
    "    @property\n",
    "    def input_ports(self) -> Sequence[TensorPort]:\n",
    "        return [embedded_question, question_length,\n",
    "                embedded_support, support_length]\n",
    "\n",
    "    @property\n",
    "    def output_ports(self) -> Sequence[TensorPort]:\n",
    "        return [XQAPorts.start_scores, XQAPorts.end_scores,\n",
    "                XQAPorts.span_prediction]\n",
    "\n",
    "    @property\n",
    "    def training_input_ports(self) -> Sequence[TensorPort]:\n",
    "        return [XQAPorts.start_scores, XQAPorts.end_scores, answer_span]\n",
    "\n",
    "    @property\n",
    "    def training_output_ports(self) -> Sequence[TensorPort]:\n",
    "        return [Ports.loss]\n",
    "\n",
    "    def create_output(self, shared_resources, emb_question, question_length,\n",
    "                      emb_support, support_length):\n",
    "        \"\"\"\n",
    "        Implements the \"core\" model: The TensorFlow subgraph which computes the\n",
    "        answer span from the embedded question and support.\n",
    "        Args:\n",
    "            emb_question: [Q, L_q, N]\n",
    "            question_length: [Q]\n",
    "            emb_support: [Q, L_s, N]\n",
    "            support_length: [Q]\n",
    "\n",
    "        Returns:\n",
    "            start_scores [B, L_s, N], end_scores [B, L_s, N], span_prediction [B, 2]\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.variable_scope(\"fast_qa\", initializer=tf.contrib.layers.xavier_initializer()):\n",
    "            dim = shared_resources.config['repr_dim']\n",
    "            # set shapes for inputs\n",
    "            emb_question.set_shape([None, None, dim])\n",
    "            emb_support.set_shape([None, None, dim])\n",
    "\n",
    "            # encode question and support\n",
    "            rnn = tf.contrib.rnn.LSTMBlockFusedCell\n",
    "            encoded_question = birnn_with_projection(dim, rnn, emb_question, question_length,\n",
    "                                                     projection_scope=\"question_proj\")\n",
    "\n",
    "            encoded_support = birnn_with_projection(dim, rnn, emb_support, support_length,\n",
    "                                                    share_rnn=True, projection_scope=\"support_proj\")\n",
    "\n",
    "            start_scores, end_scores, predicted_start_pointer, predicted_end_pointer = \\\n",
    "                self._output_layer(dim, encoded_question, question_length,\n",
    "                                   encoded_support, support_length)\n",
    "\n",
    "            span = tf.concat([predicted_start_pointer, predicted_end_pointer], 1)\n",
    "\n",
    "            return start_scores, end_scores, span\n",
    "\n",
    "    def _output_layer(self, dim, encoded_question, question_length, encoded_support, support_length):\n",
    "        \"\"\"Simple span prediction layer of our network\"\"\"\n",
    "        batch_size = tf.shape(question_length)[0]\n",
    "\n",
    "        # Computing weighted question state\n",
    "        attention_scores = tf.contrib.layers.fully_connected(encoded_question, 1,\n",
    "                                                             scope=\"question_attention\")\n",
    "        q_mask = tfutil.mask_for_lengths(question_length, batch_size)\n",
    "        attention_scores = attention_scores + tf.expand_dims(q_mask, 2)\n",
    "        question_attention_weights = tf.nn.softmax(attention_scores, 1, name=\"question_attention_weights\")\n",
    "        question_state = tf.reduce_sum(question_attention_weights * encoded_question, [1])\n",
    "\n",
    "        # Prediction\n",
    "        support_mask = tfutil.mask_for_lengths(support_length, batch_size)\n",
    "        def predict():\n",
    "            interaction = tf.expand_dims(question_state, 1) * encoded_support\n",
    "            scores = tf.layers.dense(tf.concat([interaction, encoded_support], axis=2), 1)\n",
    "            scores = tf.squeeze(scores, [2])\n",
    "            scores = scores + support_mask\n",
    "            _, predicted = tf.nn.top_k(scores, 1)\n",
    "            return scores, predicted\n",
    "\n",
    "        start_scores, predicted_start_pointer = predict()\n",
    "        end_scores, predicted_end_pointer = predict()\n",
    "\n",
    "        return start_scores, end_scores, predicted_start_pointer, predicted_end_pointer\n",
    "\n",
    "    def create_training_output(self, shared_resources, start_scores, end_scores, answer_span) \\\n",
    "            -> Sequence[TensorPort]:\n",
    "        \"\"\"Compute loss from start & end scores and the gold-standard `answer_span`.\"\"\"\n",
    "\n",
    "        start, end = [tf.squeeze(t, 1) for t in tf.split(answer_span, 2, 1)]\n",
    "\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=start_scores,\n",
    "                                                              labels=start) + \\\n",
    "               tf.nn.sparse_softmax_cross_entropy_with_logits(logits=end_scores, labels=end)\n",
    "        return [tf.reduce_mean(loss)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Module\n",
    "\n",
    "The output module converts our model predictions to `Answer` instances.\n",
    "Since our model is a standard extractive QA model and since we used the standard\n",
    "`TensorPort`s, we can reuse the existing `XQAOutputModule` rather than implementing\n",
    "our own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "As a toy example, we'll use da dataset of just one question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = [\n",
    "    (QASetting(\n",
    "        question=\"Which is it?\",\n",
    "        support=[\"While b seems plausible, answer a is correct.\"],\n",
    "        id=\"1\"),\n",
    "     [Answer(text=\"a\", span=(32, 33))])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_vocab()` function builds a random embedding matrix. Normally,\n",
    "we could load pre-trained embeddings here, such as GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "\n",
    "def build_vocab(questions):\n",
    "    \"\"\"Build a vocabulary of random vectors.\"\"\"\n",
    "\n",
    "    embedding_lookup = dict()\n",
    "    for question in questions:\n",
    "        for t in tokenize(question.question):\n",
    "            if t not in embedding_lookup:\n",
    "                embedding_lookup[t] = len(embedding_lookup)\n",
    "    embeddings = Embeddings(embedding_lookup, \n",
    "                            np.random.random([len(embedding_lookup),\n",
    "                                              embedding_dim]))\n",
    "\n",
    "    vocab = Vocab(emb=embeddings, init_from_embeddings=True)\n",
    "    return vocab\n",
    "\n",
    "questions = [q for q, _ in data_set]\n",
    "shared_resources = SharedResources(build_vocab(questions), config={'repr_dim': 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll instantiate all modules with the `shared_resources` as parameter.\n",
    "The `JTReader` needs the three modules and is ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_module = MyInputModule(shared_resources)\n",
    "model_module = MyModelModule(shared_resources)\n",
    "output_module = XQAOutputModule(shared_resources)\n",
    "reader = TFReader(shared_resources, input_module, model_module, output_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:jack.core.reader:Start training...\n",
      "INFO:jack.util.hooks:Epoch 1\tIter 1\ttrain loss 4.6132283210754395\n",
      "INFO:jack.util.hooks:Epoch 2\tIter 2\ttrain loss 4.096255302429199\n",
      "INFO:jack.util.hooks:Epoch 3\tIter 3\ttrain loss 1.8390401601791382\n",
      "INFO:jack.util.hooks:Epoch 4\tIter 4\ttrain loss 0.12551766633987427\n",
      "INFO:jack.util.hooks:Epoch 5\tIter 5\ttrain loss 0.0002952593786176294\n",
      "INFO:jack.util.hooks:Epoch 6\tIter 6\ttrain loss 2.3841856489070778e-07\n",
      "INFO:jack.util.hooks:Epoch 7\tIter 7\ttrain loss 0.0\n",
      "INFO:jack.util.hooks:Epoch 8\tIter 8\ttrain loss 2.3841855067985307e-07\n",
      "INFO:jack.util.hooks:Epoch 9\tIter 9\ttrain loss 7.271742560988059e-06\n",
      "INFO:jack.util.hooks:Epoch 10\tIter 10\ttrain loss 0.0013461976777762175\n",
      "Which is it? While b seems plausible, answer a is correct.\n",
      "0.9999980926513672, (32, 33), a\n"
     ]
    }
   ],
   "source": [
    "from jack.util.hooks import LossHook\n",
    "batch_size=1\n",
    "reader.train(tf.train.AdamOptimizer(learning_rate=0.1), \n",
    "             data_set, batch_size, max_epochs=10,\n",
    "             hooks=[LossHook(reader, iter_interval=1)])\n",
    "\n",
    "print(questions[0].question, questions[0].support[0])\n",
    "answers = reader(questions)\n",
    "print(\"{}, {}, {}\".format(answers[0].score, answers[0].span, answers[0].text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
